{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Paheli_Hugging_Face.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json"
      ],
      "metadata": {
        "id": "yxBC5TJyqQsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuzGc6Ctomm9",
        "outputId": "a6461235-b8cc-4e18-adc4-e05486290dfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/TUM/SS22/LDSI_LAB/Implementations/Paheli_data\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/My Drive/TUM/SS22/LDSI_LAB/Implementations/Paheli_data/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!ls"
      ],
      "metadata": {
        "id": "WWyrW0T5p1bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd 'text'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlwrbZN3p9lz",
        "outputId": "7bb5240c-c28f-48bd-eaf5-7469883d0d68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/TUM/SS22/LDSI_LAB/Implementations/Paheli_data/pretrained_embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VZzItLC1tR93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "Po7ir-WCqMxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "def preprocess(directory_in_str):\n",
        "  directory = os.fsencode(directory_in_str)\n",
        "  sentences = list()\n",
        "  labels = list()  \n",
        "  for file in os.listdir(directory):\n",
        "      filename = os.fsdecode(file)\n",
        "      with open(filename) as file:\n",
        "        print(filename)\n",
        "        lines = file.readlines()\n",
        "        lines = [line.rstrip() for line in lines]\n",
        "        for i in range(len(lines)):\n",
        "          a = lines[i].split('\\t')\n",
        "          sent = a[0]\n",
        "          label = a[1]\n",
        "          sentences.append(sent)\n",
        "          labels.append(label)\n",
        "      file.close()\n",
        "\n",
        "  return sentences, labels\n",
        "\n",
        "pass"
      ],
      "metadata": {
        "id": "jaFxovWRqDH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/My Drive/TUM/SS22/LDSI_LAB/Implementations/Paheli_data/pretrained_embeddings/train'\n",
        "directory_in_str = '/content/drive/My Drive/TUM/SS22/LDSI_LAB/Implementations/Paheli_data/pretrained_embeddings/train'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNAsb4cSLGJ5",
        "outputId": "bdb7b587-87fe-4eea-c78b-745d7d6843ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/TUM/SS22/LDSI_LAB/Implementations/Paheli_data/pretrained_embeddings/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences, train_labels = preprocess(directory_in_str)"
      ],
      "metadata": {
        "id": "WOlFeXIAs_-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "839b7179-e73b-445c-cd9f-f71e6ebca0ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2004_C_129.txt\n",
            "2006_A_136.txt\n",
            "1971_S_1.txt\n",
            "1987_M_123.txt\n",
            "2008_A_260.txt\n",
            "1996_B_72.txt\n",
            "2008_I_54.txt\n",
            "1994_S_246.txt\n",
            "2001_A_234.txt\n",
            "1987_S_26.txt\n",
            "1996_T_169.txt\n",
            "1987_C_108.txt\n",
            "1953_S_23.txt\n",
            "2008_S_1411.txt\n",
            "1953_L_1.txt\n",
            "1963_S_59.txt\n",
            "2000_V_80.txt\n",
            "2001_S_1131.txt\n",
            "2008_C_166.txt\n",
            "1973_S_68.txt\n",
            "2006_A_36.txt\n",
            "1977_P_19.txt\n",
            "2000_C_151.txt\n",
            "2007_S_608.txt\n",
            "1980_W_3.txt\n",
            "2007_S_632.txt\n",
            "2008_S_549.txt\n",
            "1994_M_69.txt\n",
            "1995_S_317.txt\n",
            "2005_S_388.txt\n",
            "2008_P_8.txt\n",
            "2007_B_76.txt\n",
            "1954_M_25.txt\n",
            "2007_U_18.txt\n",
            "2009_B_16.txt\n",
            "2004_I_24.txt\n",
            "2007_C_121.txt\n",
            "1978_M_13.txt\n",
            "1976_T_9.txt\n",
            "1989_A_55.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#irectory_in_str = '/content/drive/MyDrive/TUM/SS22/LDSI_LAB/Implementations/Paheli_data/text/dev'\n",
        "directory_in_str = '/content/drive/My Drive/TUM/SS22/LDSI_LAB/Implementations/Paheli_data/text/dev'\n",
        "%cd '/content/drive/My Drive/TUM/SS22/LDSI_LAB/Implementations/Paheli_data/text/dev'"
      ],
      "metadata": {
        "id": "dyoEjjgHyAoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d43985f-fa17-4da7-b709-9e113756d1ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/TUM/SS22/LDSI_LAB/Implementations/Paheli_data/pretrained_embeddings/dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd '/content/drive/MyDrive/TUM/SS22/LDSI_LAB/Implementations/Paheli_data/text/dev'"
      ],
      "metadata": {
        "id": "tqNOM05pyGvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_sentences, dev_labels = preprocess(directory_in_str)"
      ],
      "metadata": {
        "id": "tgOg3GVj0EgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef055d11-5ec3-4324-e03c-c2aa60434007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2009_S_146.txt\n",
            "2010_S_431.txt\n",
            "2011_I_16.txt\n",
            "2014_J_33.txt\n",
            "2015_J_10.txt\n",
            "2012_S_270.txt\n",
            "2010_J_55.txt\n",
            "2014_R_41.txt\n",
            "2015_S_368.txt\n",
            "2011_S_308.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Data into Json"
      ],
      "metadata": {
        "id": "vJnC7_O00U4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('train_sentences.json', 'a') as jsonfile:\n",
        "      json.dump(train_sentences, jsonfile)\n",
        "with open('train_labels.json', 'a') as jsonfile:\n",
        "      json.dump(train_labels, jsonfile)   \n",
        "with open('dev_sentences.json', 'a') as jsonfile:\n",
        "      json.dump(dev_sentences, jsonfile)\n",
        "with open('dev_labels.json', 'a') as jsonfile:\n",
        "      json.dump(dev_labels, jsonfile)   \n",
        "\n",
        "pass"
      ],
      "metadata": {
        "id": "-bGel1KY0LEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xovY41Mw0PYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fIO1SEwb00rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = json.load(open('train_sentences.json'))\n",
        "train_labels = json.load(open('train_labels.json'))\n",
        "dev_sentences = json.load(open('dev_sentences.json'))\n",
        "dev_labels = json.load(open('dev_labels.json'))"
      ],
      "metadata": {
        "id": "448Et5ni03Nw",
        "outputId": "4b5e2df5-51f1-43b5-e6c3-c93081aa949c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6587141f9bf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_sentences.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_labels.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdev_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dev_sentences.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdev_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dev_labels.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "hMtVYkv04zYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, TrainingArguments, Trainer, DataCollator\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "gPRb_8FN42V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "HRScv9CI49U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingDataset(Dataset):\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.x = train_sentences\n",
        "    self.y = train_labels\n",
        "    self.n_samples = len(train_sentences)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    input_ids = tokenizer(self.x[index])['input_ids']\n",
        "    label_ids = tokenizer(self.y[index])['input_ids']\n",
        "    return {\"input_ids\":input_ids,\"labels\":label_ids}"
      ],
      "metadata": {
        "id": "eYNUGJTB5HRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DevDataset(Dataset):\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.x = dev_sentences\n",
        "    self.y = dev_labels\n",
        "    self.n_samples = len(dev_sentences)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    input_ids = tokenizer(self.x[index])['input_ids']\n",
        "    label_ids = tokenizer(self.y[index])['input_ids']\n",
        "    return {\"input_ids\":input_ids,\"labels\":label_ids}"
      ],
      "metadata": {
        "id": "iRr5rjNJ5RMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "W809bjdz5axy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "model_name = 't5-small-paheli'\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{model_name}-seq2seq-labeling\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate = 2e-3,\n",
        "    per_device_train_batch_size = batch_size,\n",
        "    per_device_eval_batch_size = batch_size,\n",
        "    weight_decay = 0.01,\n",
        "    save_total_limit = 1,\n",
        "    save_strategy=\"no\",\n",
        "    num_train_epochs = 6,\n",
        "    predict_with_generate = True,\n",
        "    push_to_hub = False,\n",
        "    load_best_model_at_end=False,\n",
        ")\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model = model)"
      ],
      "metadata": {
        "id": "OR-jjEPY5l1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=TrainingDataset(),\n",
        "    eval_dataset=DevDataset(),\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "sI9btkG15wQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('/content/drive/MyDrive/TUM/SS22/LDSI_LAB/Implementations/Paheli_data/text/')"
      ],
      "metadata": {
        "id": "e7ufv6435xVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "TEtbH1fa5xj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spacy_tokenize(txt):\n",
        "    doc = nlp(txt)\n",
        "    tokens = list(doc)\n",
        "    clean_tokens = []\n",
        "    for t in tokens:\n",
        "        if t.pos_ == 'PUNCT':\n",
        "            pass\n",
        "        elif t.pos_ == '\\n' or t.pos == '\\n\\n':\n",
        "          pass\n",
        "        elif t.pos_ == 'NUM':\n",
        "            clean_tokens.append(f'<NUM{len(t)}>')\n",
        "        else:    \n",
        "            lower_case = t.lemma_\n",
        "            clean_tokens.append(lower_case)\n",
        "    return clean_tokens"
      ],
      "metadata": {
        "id": "WsoJmuWG5xrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "devdataset = DevDataset()\n",
        "device = 'cuda'"
      ],
      "metadata": {
        "id": "bRqPe2jXFAss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_preds = list()\n",
        "for i in range(len(devdataset)):\n",
        "  data = devdataset.__getitem__(i)\n",
        "  x = data['input_ids']\n",
        "  y = data['labels']\n",
        "  pred = model.generate(input_ids = torch.tensor(x).to(device).view(1,-1))\n",
        "  pred_decoded = spacy_tokenize(tokenizer.decode(pred.squeeze(0)))\n",
        "  pred_decoded_clean = pred_decoded[3][:-3]\n",
        "  #print(pred_decoded_clean)\n",
        "  dev_preds.append(pred_decoded_clean)"
      ],
      "metadata": {
        "id": "CcekEexPE56a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1_weighted = f1_score(dev_labels, dev_preds, average='weighted')\n",
        "f1_macro = f1_score(dev_labels, dev_preds, average='macro')\n",
        "f1_micro = f1_score(dev_labels, dev_preds, average='micro')\n"
      ],
      "metadata": {
        "id": "e9KJk3FTGgsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Weighted AVG F1: '+str(f1_weighted))\n",
        "print('Macro AVG F1: '+str(f1_macro))\n",
        "print('Micro AVG F1: '+str(f1_micro))"
      ],
      "metadata": {
        "id": "P45_W7NOFSNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### For training set:"
      ],
      "metadata": {
        "id": "MHEWK7E6G0A8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "traindataset = TrainingDataset()\n"
      ],
      "metadata": {
        "id": "LhIuKUFgG3Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_preds = list()\n",
        "for i in range(len(traindataset)):\n",
        "  data = traindataset.__getitem__(i)\n",
        "  x = data['input_ids']\n",
        "  y = data['labels']\n",
        "  pred = model.generate(input_ids = torch.tensor(x).to(device).view(1,-1))\n",
        "  pred_decoded = spacy_tokenize(tokenizer.decode(pred.squeeze(0)))\n",
        "  pred_decoded_clean = pred_decoded[3][:-3]\n",
        "  #print(pred_decoded_clean)\n",
        "  train_preds.append(pred_decoded_clean)"
      ],
      "metadata": {
        "id": "qFyj_jEcG_PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1_weighted = f1_score(train_labels, train_preds, average='weighted')\n",
        "f1_macro = f1_score(train_labels, train_preds, average='macro')\n",
        "f1_micro = f1_score(train_labels, train_preds, average='micro')"
      ],
      "metadata": {
        "id": "f8xjznuSHEow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Weighted AVG F1: '+str(f1_weighted))\n",
        "print('Macro AVG F1: '+str(f1_macro))\n",
        "print('Micro AVG F1: '+str(f1_micro))"
      ],
      "metadata": {
        "id": "BtB4ZZWVIEoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UW1oxj-yLTyJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}